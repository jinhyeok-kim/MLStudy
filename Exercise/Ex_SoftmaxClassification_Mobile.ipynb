{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoftmaxClassification_Mobile\n",
    "\n",
    "이전 Iris Excercise 코드를 복습하면서, Data Type을 다루는 방법을 조금 연구하고 새로운 DataSet을 구해 학습해보았다.  \n",
    "Iris보다 x value가 많아지고 data의 수도 많아져서 학습하는데 시간도 걸렸는데 정확도도 떨어졌다.  \n",
    "<br>\n",
    "이 실습을 하고 김성훈 교수님의 Lecture07을 들었는데, running rate는 경험적으로 알게되었고, regularization의 문제일수도 있다는 생각이 들었다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:     0\tLoss: 1.380\tAcc: 25.23%\n",
      "Step: 10000\tLoss: 1.118\tAcc: 61.95%\n",
      "Step: 20000\tLoss: 1.101\tAcc: 63.23%\n",
      "Step: 30000\tLoss: 1.091\tAcc: 64.36%\n",
      "Step: 40000\tLoss: 1.086\tAcc: 64.87%\n",
      "Step: 50000\tLoss: 1.081\tAcc: 65.23%\n",
      "Step: 60000\tLoss: 1.078\tAcc: 65.33%\n",
      "Step: 70000\tLoss: 1.075\tAcc: 65.33%\n",
      "Step: 80000\tLoss: 1.073\tAcc: 65.69%\n",
      "Step: 90000\tLoss: 1.071\tAcc: 66.10%\n",
      "Step: 100000\tLoss: 1.070\tAcc: 66.26%\n",
      "Step: 110000\tLoss: 1.068\tAcc: 66.46%\n",
      "Step: 120000\tLoss: 1.067\tAcc: 66.36%\n",
      "Step: 130000\tLoss: 1.066\tAcc: 66.72%\n",
      "Step: 140000\tLoss: 1.065\tAcc: 66.97%\n",
      "Step: 150000\tLoss: 1.064\tAcc: 67.23%\n",
      "Step: 160000\tLoss: 1.063\tAcc: 67.23%\n",
      "Step: 170000\tLoss: 1.062\tAcc: 67.23%\n",
      "Step: 180000\tLoss: 1.062\tAcc: 67.28%\n",
      "Step: 190000\tLoss: 1.061\tAcc: 67.44%\n",
      "Step: 200000\tLoss: 1.060\tAcc: 67.59%\n",
      "Step: 210000\tLoss: 1.059\tAcc: 67.74%\n",
      "Step: 220000\tLoss: 1.059\tAcc: 67.90%\n",
      "Step: 230000\tLoss: 1.058\tAcc: 68.10%\n",
      "Step: 240000\tLoss: 1.057\tAcc: 68.10%\n",
      "Step: 250000\tLoss: 1.057\tAcc: 68.00%\n",
      "Step: 260000\tLoss: 1.056\tAcc: 68.15%\n",
      "Step: 270000\tLoss: 1.056\tAcc: 68.21%\n",
      "Step: 280000\tLoss: 1.055\tAcc: 68.41%\n",
      "Step: 290000\tLoss: 1.055\tAcc: 68.62%\n",
      "Step: 300000\tLoss: 1.054\tAcc: 68.72%\n",
      "Step: 310000\tLoss: 1.053\tAcc: 68.67%\n",
      "Step: 320000\tLoss: 1.053\tAcc: 68.62%\n",
      "Step: 330000\tLoss: 1.052\tAcc: 68.72%\n",
      "Step: 340000\tLoss: 1.052\tAcc: 68.77%\n",
      "Step: 350000\tLoss: 1.052\tAcc: 68.82%\n",
      "Step: 360000\tLoss: 1.051\tAcc: 68.92%\n",
      "Step: 370000\tLoss: 1.051\tAcc: 69.13%\n",
      "Step: 380000\tLoss: 1.050\tAcc: 69.23%\n",
      "Step: 390000\tLoss: 1.050\tAcc: 69.18%\n",
      "Step: 400000\tLoss: 1.049\tAcc: 69.23%\n",
      "Step: 410000\tLoss: 1.049\tAcc: 69.28%\n",
      "Step: 420000\tLoss: 1.049\tAcc: 69.23%\n",
      "Step: 430000\tLoss: 1.048\tAcc: 69.23%\n",
      "Step: 440000\tLoss: 1.048\tAcc: 69.38%\n",
      "Step: 450000\tLoss: 1.047\tAcc: 69.49%\n",
      "Step: 460000\tLoss: 1.047\tAcc: 69.59%\n",
      "Step: 470000\tLoss: 1.047\tAcc: 69.74%\n",
      "Step: 480000\tLoss: 1.046\tAcc: 69.79%\n",
      "Step: 490000\tLoss: 1.046\tAcc: 69.85%\n",
      "Step: 500000\tLoss: 1.046\tAcc: 69.85%\n",
      "Step: 510000\tLoss: 1.045\tAcc: 69.90%\n",
      "Step: 520000\tLoss: 1.045\tAcc: 69.90%\n",
      "Step: 530000\tLoss: 1.045\tAcc: 69.95%\n",
      "Step: 540000\tLoss: 1.044\tAcc: 69.95%\n",
      "Step: 550000\tLoss: 1.044\tAcc: 70.00%\n",
      "Step: 560000\tLoss: 1.044\tAcc: 70.10%\n",
      "Step: 570000\tLoss: 1.043\tAcc: 70.10%\n",
      "Step: 580000\tLoss: 1.043\tAcc: 70.15%\n",
      "Step: 590000\tLoss: 1.043\tAcc: 70.21%\n",
      "Step: 600000\tLoss: 1.042\tAcc: 70.26%\n",
      "Step: 610000\tLoss: 1.042\tAcc: 70.26%\n",
      "Step: 620000\tLoss: 1.042\tAcc: 70.21%\n",
      "Step: 630000\tLoss: 1.042\tAcc: 70.26%\n",
      "Step: 640000\tLoss: 1.041\tAcc: 70.26%\n",
      "Step: 650000\tLoss: 1.041\tAcc: 70.26%\n",
      "Step: 660000\tLoss: 1.041\tAcc: 70.31%\n",
      "Step: 670000\tLoss: 1.040\tAcc: 70.31%\n",
      "Step: 680000\tLoss: 1.040\tAcc: 70.41%\n",
      "Step: 690000\tLoss: 1.040\tAcc: 70.46%\n",
      "Step: 700000\tLoss: 1.040\tAcc: 70.36%\n",
      "Step: 710000\tLoss: 1.039\tAcc: 70.46%\n",
      "Step: 720000\tLoss: 1.039\tAcc: 70.41%\n",
      "Step: 730000\tLoss: 1.039\tAcc: 70.36%\n",
      "Step: 740000\tLoss: 1.039\tAcc: 70.46%\n",
      "Step: 750000\tLoss: 1.038\tAcc: 70.62%\n",
      "Step: 760000\tLoss: 1.038\tAcc: 70.56%\n",
      "Step: 770000\tLoss: 1.038\tAcc: 70.56%\n",
      "Step: 780000\tLoss: 1.038\tAcc: 70.51%\n",
      "Step: 790000\tLoss: 1.038\tAcc: 70.51%\n",
      "Step: 800000\tLoss: 1.037\tAcc: 70.46%\n",
      "Step: 810000\tLoss: 1.037\tAcc: 70.46%\n",
      "Step: 820000\tLoss: 1.037\tAcc: 70.41%\n",
      "Step: 830000\tLoss: 1.037\tAcc: 70.51%\n",
      "Step: 840000\tLoss: 1.036\tAcc: 70.51%\n",
      "Step: 850000\tLoss: 1.036\tAcc: 70.67%\n",
      "Step: 860000\tLoss: 1.036\tAcc: 70.67%\n",
      "Step: 870000\tLoss: 1.036\tAcc: 70.82%\n",
      "Step: 880000\tLoss: 1.036\tAcc: 70.82%\n",
      "Step: 890000\tLoss: 1.035\tAcc: 70.87%\n",
      "Step: 900000\tLoss: 1.035\tAcc: 70.82%\n",
      "Step: 910000\tLoss: 1.035\tAcc: 70.97%\n",
      "Step: 920000\tLoss: 1.035\tAcc: 71.08%\n",
      "Step: 930000\tLoss: 1.035\tAcc: 71.03%\n",
      "Step: 940000\tLoss: 1.034\tAcc: 71.08%\n",
      "Step: 950000\tLoss: 1.034\tAcc: 71.08%\n",
      "Step: 960000\tLoss: 1.034\tAcc: 71.13%\n",
      "Step: 970000\tLoss: 1.034\tAcc: 71.13%\n",
      "Step: 980000\tLoss: 1.034\tAcc: 71.13%\n",
      "Step: 990000\tLoss: 1.034\tAcc: 71.18%\n",
      "Step: 1000000\tLoss: 1.033\tAcc: 71.18%\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "Accuracy: 70.00%\n",
      "\n",
      "The mobile which has [[ 909.     1.     0.7    1.     7.     0.    47.     0.1  146.     3.\n",
      "     9.   410.  1643.  1244.    14.    10.    16.     0.     1.     0. ]] may be price :  very high cost\n",
      "Incorrect!\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "The mobile which has  [[ 790.     1.     1.     1.     9.     1.    45.     0.1  200.     6.\n",
      "    20.  1068.  1577.  3184.    17.     5.    13.     1.     0.     1. ]] maybe medium cost\n",
      "\n",
      "The mobile which has  [[1711.     1.     0.5    1.     1.     1.    12.     1.    97.     1.\n",
      "    18.   855.  1199.  2461.     9.     4.     9.     1.     1.     0. ]] maybe high cost\n",
      "\n",
      "The mobile which has  [[1544.     0.     1.     1.     2.     1.    64.     0.3  193.     1.\n",
      "     6.   595.   675.  1715.     8.     0.    16.     1.     1.     1. ]] maybe low cost\n",
      "\n",
      "The mobile which has  [[ 951.     1.     2.4    0.     1.     1.    24.     0.1  147.     8.\n",
      "    20.   897.  1964.  3391.    19.     4.     7.     1.     0.     0. ]] maybe medium cost\n",
      "\n",
      "The mobile which has  [[ 639.     0.     1.3    0.     5.     0.    28.     0.9  113.     7.\n",
      "    16.   983.  1369.   391.     6.     2.    12.     0.     0.     1. ]] maybe very high cost\n",
      "\n",
      "The mobile which has  [[1040.     1.     0.5    1.     3.     0.    41.     0.8  116.     3.\n",
      "    18.   299.   608.  1006.    12.     7.    18.     0.     0.     1. ]] maybe very high cost\n",
      "\n",
      "The mobile which has  [[1066.     1.     0.9    0.     4.     0.    43.     0.4  101.     6.\n",
      "    11.   876.   880.  3856.    14.     0.     6.     1.     0.     1. ]] maybe high cost\n",
      "\n",
      "The mobile which has  [[1075.     1.     1.9    0.     0.     0.    36.     0.6  127.     4.\n",
      "     3.  1335.  1427.  3870.     8.     3.    20.     1.     1.     0. ]] maybe high cost\n",
      "\n",
      "The mobile which has  [[ 873.     1.     0.5    0.     4.     1.    30.     0.2  175.     6.\n",
      "    18.   237.  1531.  3655.    15.    13.    15.     1.     0.     0. ]] maybe medium cost\n",
      "\n",
      "The mobile which has  [[ 762.     1.     0.6    1.    11.     1.    39.     0.7  164.     1.\n",
      "    14.  1065.  1686.  2271.    12.     7.     8.     1.     1.     0. ]] maybe low cost\n",
      "\n",
      "The mobile which has  [[1730.     0.     1.3    1.     2.     1.    48.     0.4  198.     2.\n",
      "    13.  1445.  1934.  2452.     8.     2.    11.     1.     1.     1. ]] maybe medium cost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:68: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "keys = ['battery_power', 'blue','clock_speed', 'dual_sim', 'fc','four_g','int_memory','m_dep','mobile_wt','n_cores','pc','px_height','px_width','ram','sc_h','sc_w','talk_time','three_g','touch_screen','wifi']\n",
    "data = pd.read_csv(\"mobile-train.csv\")\n",
    "\n",
    "\n",
    "\n",
    "test = pd.read_csv(\"mobile-test.csv\")\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "price_range = list(data['price_range'].unique())\n",
    "data['class'] = data['price_range'].map(lambda x: np.eye(len(price_range))[price_range.index(x)])\n",
    "\n",
    "testset = data.sample(50)\n",
    "trainset = data.drop(testset.index)\n",
    "\n",
    "nb_classes = len(species)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,len(keys)])\n",
    "Y = tf.placeholder(tf.float32, [None,len(price_range)])\n",
    "\n",
    "W = tf.Variable(tf.zeros([len(keys),len(price_range)]), name= \"Weight\")\n",
    "b = tf.Variable(tf.zeros([len(price_range)]), name= \"Bias\")\n",
    "\n",
    "H = tf.nn.softmax(tf.matmul(X, W)+b)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=H, labels=Y, name=\"Cross_Entropy\"))\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-5).minimize(cross_entropy)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(H,1),tf.argmax(Y,1)), tf.float32))\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "trainset_class = [y for y in trainset['class'].values]\n",
    "\n",
    "for step in xrange(1000001):\n",
    "    \n",
    "    sess.run(optimizer, feed_dict={X: trainset[keys].values,\n",
    "                                  Y: trainset_class})\n",
    "    if step%10000 == 0:\n",
    "        loss, acc = sess.run([cross_entropy, accuracy], feed_dict={X: trainset[keys].values,\n",
    "                                  Y: trainset_class})\n",
    "        print \"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc)\n",
    "\n",
    "print \"\\n------------------------------------------------------------\\n\"\n",
    "\n",
    "accu =  sess.run(accuracy, feed_dict={X: testset[keys].values,\n",
    "                                   Y: [y for y in testset['class'].values]})\n",
    "\n",
    "print \"\\nAccuracy: {:.2%}\\n\".format(accu)\n",
    "\n",
    "price = [\"low cost\", \"medium cost\", \"high cost\", \"very high cost\"]\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "sample = data.sample(1)\n",
    "\n",
    "result = price[np.argmax(sess.run(H, feed_dict = {X: sample[keys].values}))]\n",
    "\n",
    "print \"The mobile which has\", sample[keys].values, \"may be price : \", result\n",
    "\n",
    "if result == sample['price_range'].values:\n",
    "    print \"Correct!\"\n",
    "\n",
    "else:\n",
    "    print \"Incorrect!\"\n",
    "    \n",
    "print \"\\n------------------------------------------------------------\\n\"\n",
    "for j in range(11):\n",
    "    sample = test.sample(1)\n",
    "    result = price[np.argmax(sess.run(H, feed_dict = {X: sample[keys].values}))]\n",
    "    print \"\\nThe mobile which has \", sample[keys].values,\"maybe\", result\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
