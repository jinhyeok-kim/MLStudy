{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Lab09-1 : Neural Net for XOR\n",
    "\n",
    "## XOR data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1352038 [[-1.9877452]\n",
      " [ 0.876346 ]]\n",
      "1000 0.69314814 [[-0.00074433]\n",
      " [ 0.00547151]]\n",
      "2000 0.69314724 [[4.0491446e-05]\n",
      " [5.2262567e-05]]\n",
      "3000 0.6931472 [[9.0352643e-07]\n",
      " [9.0421275e-07]]\n",
      "4000 0.6931472 [[1.3313471e-07]\n",
      " [1.3382103e-07]]\n",
      "5000 0.6931472 [[1.3313471e-07]\n",
      " [1.3382103e-07]]\n",
      "6000 0.6931472 [[1.3313471e-07]\n",
      " [1.3382103e-07]]\n",
      "7000 0.6931472 [[1.3313471e-07]\n",
      " [1.3382103e-07]]\n",
      "8000 0.6931472 [[1.3313471e-07]\n",
      " [1.3382103e-07]]\n",
      "9000 0.6931472 [[1.3313471e-07]\n",
      " [1.3382103e-07]]\n",
      "10000 0.6931472 [[1.3313471e-07]\n",
      " [1.3382103e-07]]\n",
      "\n",
      "Hypothesis:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "# hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "\n",
    "# cost/Loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print step, sess.run(cost, feed_dict={X: x_data, Y: y_data}) , sess.run(W)\n",
    "            \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    print \"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 모델은 100만번 돌려도 맞지 않는다.\n",
    "\n",
    "간단한 모델인거 같지만 하나의 gate로는 xor 문제를 풀지를 못한다.\n",
    "\n",
    "그래서 **Neural Net** 를 이용한다!\n",
    "\n",
    "기존의 하나의 layer로는 XOR을 해결 할 수 없어 여러 layer를 이용한다!\n",
    "\n",
    "- 기존의 하나의 layer\n",
    "\n",
    "```python\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "# hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "```\n",
    "<br>\n",
    "\n",
    "- 2개의 layer\n",
    "\n",
    "```python\n",
    "\n",
    "# weight의 크기를 잘 정해줘야한다!\n",
    "# [입력, 출력] x1, x2 두개가 들어가므로 입력 : 2, 출력은 정하면 된다. 이 땐 그냥 임의로 out을 2로 정해준다.\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name = 'weight1')\n",
    "# bias는 out의 크기와 같게 정해준다.\n",
    "b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# [입력, 출력] 입력: layer1의 결과의 크기, 출력: 1\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name = 'weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.73526436 [[0.09393647]\n",
      " [0.64000106]]\n",
      "1000 0.6864102 [[0.09393647]\n",
      " [0.64000106]]\n",
      "2000 0.6584469 [[0.09393647]\n",
      " [0.64000106]]\n",
      "3000 0.55705273 [[0.09393647]\n",
      " [0.64000106]]\n",
      "4000 0.44817588 [[0.09393647]\n",
      " [0.64000106]]\n",
      "5000 0.40351662 [[0.09393647]\n",
      " [0.64000106]]\n",
      "6000 0.38443846 [[0.09393647]\n",
      " [0.64000106]]\n",
      "7000 0.374473 [[0.09393647]\n",
      " [0.64000106]]\n",
      "8000 0.3684831 [[0.09393647]\n",
      " [0.64000106]]\n",
      "9000 0.36452746 [[0.09393647]\n",
      " [0.64000106]]\n",
      "10000 0.36173722 [[0.09393647]\n",
      " [0.64000106]]\n",
      "\n",
      "Hypothesis:  [[0.50801504]\n",
      " [0.9849849 ]\n",
      " [0.49227557]\n",
      " [0.01370145]] \n",
      "Correct:  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name = 'weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name = 'bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name = 'weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# cost/Loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print step, sess.run(cost, feed_dict={X: x_data, Y: y_data}) , sess.run(W)\n",
    "            \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    print \"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide NN for XOR\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# weight의 크기를 잘 정해줘야한다!\n",
    "# [입력, 출력] 입력: 2, 출력 : 10\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name = 'weight1')\n",
    "# bias는 out의 크기와 같게 정해준다.\n",
    "b1 = tf.Variable(tf.random_normal([10]), name = 'bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# [입력, 출력] 입력: layer1의 결과의 크기, 출력: 1\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), name = 'weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.7281829 [[ 0.49218276]\n",
      " [-1.1729602 ]]\n",
      "1000 0.58243835 [[ 0.49218276]\n",
      " [-1.1729602 ]]\n",
      "2000 0.18482532 [[ 0.49218276]\n",
      " [-1.1729602 ]]\n",
      "3000 0.06352152 [[ 0.49218276]\n",
      " [-1.1729602 ]]\n",
      "4000 0.033852883 [[ 0.49218276]\n",
      " [-1.1729602 ]]\n",
      "5000 0.022124346 [[ 0.49218276]\n",
      " [-1.1729602 ]]\n",
      "6000 0.016103934 [[ 0.49218276]\n",
      " [-1.1729602 ]]\n",
      "7000 0.012513276 [[ 0.49218276]\n",
      " [-1.1729602 ]]\n",
      "8000 0.010155875 [[ 0.49218276]\n",
      " [-1.1729602 ]]\n",
      "9000 0.008501915 [[ 0.49218276]\n",
      " [-1.1729602 ]]\n",
      "10000 0.007283694 [[ 0.49218276]\n",
      " [-1.1729602 ]]\n",
      "\n",
      "Hypothesis:  [[0.00416655]\n",
      " [0.99202055]\n",
      " [0.992827  ]\n",
      " [0.00970184]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name = 'weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name = 'bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), name = 'weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# cost/Loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print step, sess.run(cost, feed_dict={X: x_data, Y: y_data}) , sess.run(W)\n",
    "            \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    print \"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep NN for XOR\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name = 'weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name = 'bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 10]), name = 'weight2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name = 'bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10, 10]), name = 'weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name = 'bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10, 1]), name = 'weight4')\n",
    "b4 = tf.Variable(tf.random_normal([1]), name = 'bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.8222151 [[ 0.5991088 ]\n",
      " [-0.50057256]]\n",
      "1000 0.4822173 [[ 0.5991088 ]\n",
      " [-0.50057256]]\n",
      "2000 0.024701329 [[ 0.5991088 ]\n",
      " [-0.50057256]]\n",
      "3000 0.008234529 [[ 0.5991088 ]\n",
      " [-0.50057256]]\n",
      "4000 0.004635028 [[ 0.5991088 ]\n",
      " [-0.50057256]]\n",
      "5000 0.0031521334 [[ 0.5991088 ]\n",
      " [-0.50057256]]\n",
      "6000 0.0023610378 [[ 0.5991088 ]\n",
      " [-0.50057256]]\n",
      "7000 0.0018747676 [[ 0.5991088 ]\n",
      " [-0.50057256]]\n",
      "8000 0.0015478749 [[ 0.5991088 ]\n",
      " [-0.50057256]]\n",
      "9000 0.0013140341 [[ 0.5991088 ]\n",
      " [-0.50057256]]\n",
      "10000 0.0011390656 [[ 0.5991088 ]\n",
      " [-0.50057256]]\n",
      "\n",
      "Hypothesis:  [[9.2193374e-04]\n",
      " [9.9853885e-01]\n",
      " [9.9937564e-01]\n",
      " [1.5459764e-03]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name = 'weight1')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name = 'bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 10]), name = 'weight2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name = 'bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10, 10]), name = 'weight3')\n",
    "b3 = tf.Variable(tf.random_normal([10]), name = 'bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10, 1]), name = 'weight4')\n",
    "b4 = tf.Variable(tf.random_normal([1]), name = 'bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "# cost/Loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print step, sess.run(cost, feed_dict={X: x_data, Y: y_data}) , sess.run(W)\n",
    "            \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "    print \"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
