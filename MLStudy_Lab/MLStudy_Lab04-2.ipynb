{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Lab04-2 : Loading Data from File\n",
    "\n",
    "## Loading data from file\n",
    "\n",
    "Data가 많아지면서 소스코드에 data를 올리기엔 부담이 생긴다.  \n",
    "그래서 Data를 load 해오는데 이 때 csv 파일을 주로 이용한다.  \n",
    "<br>\n",
    "가장 간단하게 load해오는 방법은 numpy에 loadtxt를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 3) [[ 73.  80.  75.]\n",
      " [ 93.  88.  93.]\n",
      " [ 89.  91.  90.]\n",
      " [ 96.  98. 100.]\n",
      " [ 73.  66.  70.]\n",
      " [ 53.  46.  55.]\n",
      " [ 69.  74.  77.]\n",
      " [ 47.  56.  60.]\n",
      " [ 87.  79.  90.]\n",
      " [ 79.  70.  88.]\n",
      " [ 69.  70.  73.]\n",
      " [ 70.  65.  74.]\n",
      " [ 93.  95.  91.]\n",
      " [ 79.  80.  73.]\n",
      " [ 70.  73.  78.]\n",
      " [ 93.  89.  96.]\n",
      " [ 78.  75.  68.]\n",
      " [ 81.  90.  93.]\n",
      " [ 88.  92.  86.]\n",
      " [ 78.  83.  77.]\n",
      " [ 82.  86.  90.]\n",
      " [ 86.  82.  89.]\n",
      " [ 78.  83.  85.]\n",
      " [ 76.  83.  71.]\n",
      " [ 96.  93.  95.]] 25\n",
      "(25, 1) [[152.]\n",
      " [185.]\n",
      " [180.]\n",
      " [196.]\n",
      " [142.]\n",
      " [101.]\n",
      " [149.]\n",
      " [115.]\n",
      " [175.]\n",
      " [164.]\n",
      " [141.]\n",
      " [141.]\n",
      " [184.]\n",
      " [152.]\n",
      " [148.]\n",
      " [192.]\n",
      " [147.]\n",
      " [183.]\n",
      " [177.]\n",
      " [159.]\n",
      " [177.]\n",
      " [175.]\n",
      " [175.]\n",
      " [149.]\n",
      " [192.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# loadtxt 이용하기 단점은 전체의 data가 같은 data type이어야한다.\n",
    "xy = np.loadtxt('data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "# x와 y를 나누어야하는데 이 때 python의 기능 중 slice가 필요하다.\n",
    "# python Slicing 공부하기\n",
    "# numpy에서는 더욱 막강한 slicing이 가능해진다! 찾아서 공부하기\n",
    "\n",
    "#: 전체 행을 다 가져온다, 0:-1 마지막꺼 전까지 가져오겠다.\n",
    "x_data = xy[:, 0:-1]\n",
    "#: 전체 행을 가져오는데 마지막 행만 가져오겠다.\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print x_data.shape, x_data, len(x_data)\n",
    "print y_data.shape, y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost :  1111.3954 \n",
      " Prediction: \n",
      "[[119.41473 ]\n",
      " [147.72069 ]\n",
      " [143.19298 ]\n",
      " [158.96211 ]\n",
      " [111.104576]\n",
      " [ 86.83268 ]\n",
      " [122.1238  ]\n",
      " [ 94.84571 ]\n",
      " [142.59616 ]\n",
      " [138.96619 ]\n",
      " [115.81169 ]\n",
      " [117.12813 ]\n",
      " [144.98465 ]\n",
      " [116.46243 ]\n",
      " [123.62691 ]\n",
      " [152.36145 ]\n",
      " [108.553894]\n",
      " [147.59314 ]\n",
      " [137.09581 ]\n",
      " [122.70947 ]\n",
      " [142.8308  ]\n",
      " [141.1864  ]\n",
      " [134.96037 ]\n",
      " [113.484505]\n",
      " [151.07198 ]] \n",
      "\n",
      "100 Cost :  14.840899 \n",
      " Prediction: \n",
      "[[150.65323]\n",
      " [185.09476]\n",
      " [180.11232]\n",
      " [199.10275]\n",
      " [139.62119]\n",
      " [107.73281]\n",
      " [152.1185 ]\n",
      " [117.01439]\n",
      " [177.40277]\n",
      " [171.04135]\n",
      " [144.72797]\n",
      " [145.53854]\n",
      " [183.19336]\n",
      " [148.30978]\n",
      " [153.72636]\n",
      " [190.23903]\n",
      " [138.91275]\n",
      " [183.58704]\n",
      " [173.5595 ]\n",
      " [155.34213]\n",
      " [178.00368]\n",
      " [176.19354]\n",
      " [168.52869]\n",
      " [145.13452]\n",
      " [189.86458]] \n",
      "\n",
      "200 Cost :  14.1101675 \n",
      " Prediction: \n",
      "[[150.7623 ]\n",
      " [185.06221]\n",
      " [180.16988]\n",
      " [199.101  ]\n",
      " [139.60883]\n",
      " [107.60502]\n",
      " [152.06718]\n",
      " [116.9113 ]\n",
      " [177.2556 ]\n",
      " [170.74788]\n",
      " [144.69594]\n",
      " [145.41002]\n",
      " [183.31598]\n",
      " [148.49289]\n",
      " [153.64323]\n",
      " [190.15968]\n",
      " [139.11461]\n",
      " [183.5227 ]\n",
      " [173.71185]\n",
      " [155.48445]\n",
      " [177.94298]\n",
      " [176.11053]\n",
      " [168.50307]\n",
      " [145.39209]\n",
      " [189.88693]] \n",
      "\n",
      "300 Cost :  13.440647 \n",
      " Prediction: \n",
      "[[150.86635 ]\n",
      " [185.0314  ]\n",
      " [180.22491 ]\n",
      " [199.09921 ]\n",
      " [139.59749 ]\n",
      " [107.483086]\n",
      " [152.01768 ]\n",
      " [116.811905]\n",
      " [177.11514 ]\n",
      " [170.46732 ]\n",
      " [144.66519 ]\n",
      " [145.28722 ]\n",
      " [183.43338 ]\n",
      " [148.6683  ]\n",
      " [153.56339 ]\n",
      " [190.08394 ]\n",
      " [139.30827 ]\n",
      " [183.46046 ]\n",
      " [173.8576  ]\n",
      " [155.62048 ]\n",
      " [177.88457 ]\n",
      " [176.03128 ]\n",
      " [168.47818 ]\n",
      " [145.63843 ]\n",
      " [189.90858 ]] \n",
      "\n",
      "400 Cost :  12.827153 \n",
      " Prediction: \n",
      "[[150.96559]\n",
      " [185.00224]\n",
      " [180.27756]\n",
      " [199.0974 ]\n",
      " [139.58711]\n",
      " [107.36673]\n",
      " [151.96992]\n",
      " [116.71605]\n",
      " [176.98114]\n",
      " [170.19911]\n",
      " [144.63567]\n",
      " [145.1699 ]\n",
      " [183.54576]\n",
      " [148.83635]\n",
      " [153.48668]\n",
      " [190.01167]\n",
      " [139.4941 ]\n",
      " [183.40022]\n",
      " [173.99702]\n",
      " [155.75053]\n",
      " [177.82834]\n",
      " [175.95567]\n",
      " [168.45403]\n",
      " [145.87404]\n",
      " [189.92958]] \n",
      "\n",
      "500 Cost :  12.264978 \n",
      " Prediction: \n",
      "[[151.06026]\n",
      " [184.97469]\n",
      " [180.32793]\n",
      " [199.09557]\n",
      " [139.57768]\n",
      " [107.2557 ]\n",
      " [151.92387]\n",
      " [116.62362]\n",
      " [176.8533 ]\n",
      " [169.9427 ]\n",
      " [144.60736]\n",
      " [145.05786]\n",
      " [183.65338]\n",
      " [148.99736]\n",
      " [153.41301]\n",
      " [189.94273]\n",
      " [139.6724 ]\n",
      " [183.34195]\n",
      " [174.13039]\n",
      " [155.87486]\n",
      " [177.77425]\n",
      " [175.8835 ]\n",
      " [168.43059]\n",
      " [146.09941]\n",
      " [189.94994]] \n",
      "\n",
      "600 Cost :  11.7498 \n",
      " Prediction: \n",
      "[[151.15056]\n",
      " [184.94864]\n",
      " [180.37608]\n",
      " [199.09372]\n",
      " [139.5691 ]\n",
      " [107.14978]\n",
      " [151.87943]\n",
      " [116.53446]\n",
      " [176.73134]\n",
      " [169.69762]\n",
      " [144.58018]\n",
      " [144.95082]\n",
      " [183.75642]\n",
      " [149.15163]\n",
      " [153.34224]\n",
      " [189.87697]\n",
      " [139.84349]\n",
      " [183.28555]\n",
      " [174.25798]\n",
      " [155.99373]\n",
      " [177.72218]\n",
      " [175.81462]\n",
      " [168.40782]\n",
      " [146.31496]\n",
      " [189.96968]] \n",
      "\n",
      "700 Cost :  11.277674 \n",
      " Prediction: \n",
      "[[151.23668]\n",
      " [184.924  ]\n",
      " [180.42213]\n",
      " [199.09184]\n",
      " [139.56136]\n",
      " [107.04873]\n",
      " [151.83653]\n",
      " [116.44847]\n",
      " [176.61497]\n",
      " [169.4633 ]\n",
      " [144.5541 ]\n",
      " [144.84857]\n",
      " [183.85507]\n",
      " [149.29942]\n",
      " [153.27425]\n",
      " [189.81421]\n",
      " [140.00766]\n",
      " [183.23094]\n",
      " [174.38002]\n",
      " [156.10732]\n",
      " [177.67203]\n",
      " [175.74887]\n",
      " [168.38571]\n",
      " [146.52107]\n",
      " [189.98882]] \n",
      "\n",
      "800 Cost :  10.844954 \n",
      " Prediction: \n",
      "[[151.31883]\n",
      " [184.90076]\n",
      " [180.46619]\n",
      " [199.08998]\n",
      " [139.55438]\n",
      " [106.95233]\n",
      " [151.79515]\n",
      " [116.36553]\n",
      " [176.50398]\n",
      " [169.23935]\n",
      " [144.52907]\n",
      " [144.75092]\n",
      " [183.94952]\n",
      " [149.44104]\n",
      " [153.20892]\n",
      " [189.75435]\n",
      " [140.16522]\n",
      " [183.1781 ]\n",
      " [174.4968 ]\n",
      " [156.21594]\n",
      " [177.62376]\n",
      " [175.68617]\n",
      " [168.36426]\n",
      " [146.71826]\n",
      " [190.00739]] \n",
      "\n",
      "900 Cost :  10.448346 \n",
      " Prediction: \n",
      "[[151.39714]\n",
      " [184.87877]\n",
      " [180.50832]\n",
      " [199.08809]\n",
      " [139.54814]\n",
      " [106.86036]\n",
      " [151.7552 ]\n",
      " [116.2855 ]\n",
      " [176.3981 ]\n",
      " [169.02525]\n",
      " [144.50502]\n",
      " [144.65764]\n",
      " [184.03996]\n",
      " [149.57674]\n",
      " [153.14616]\n",
      " [189.69722]\n",
      " [140.31642]\n",
      " [183.12694]\n",
      " [174.60849]\n",
      " [156.31975]\n",
      " [177.57729]\n",
      " [175.62633]\n",
      " [168.3434 ]\n",
      " [146.90683]\n",
      " [190.02538]] \n",
      "\n",
      "1000 Cost :  10.084781 \n",
      " Prediction: \n",
      "[[151.47183]\n",
      " [184.85803]\n",
      " [180.5486 ]\n",
      " [199.0862 ]\n",
      " [139.54259]\n",
      " [106.77263]\n",
      " [151.71664]\n",
      " [116.2083 ]\n",
      " [176.29712]\n",
      " [168.82059]\n",
      " [144.48195]\n",
      " [144.56854]\n",
      " [184.12654]\n",
      " [149.70676]\n",
      " [153.08585]\n",
      " [189.64276]\n",
      " [140.46155]\n",
      " [183.0774 ]\n",
      " [174.71533]\n",
      " [156.41898]\n",
      " [177.53253]\n",
      " [175.56921]\n",
      " [168.32315]\n",
      " [147.08717]\n",
      " [190.04282]] \n",
      "\n",
      "1100 Cost :  9.751506 \n",
      " Prediction: \n",
      "[[151.54305]\n",
      " [184.83847]\n",
      " [180.58713]\n",
      " [199.0843 ]\n",
      " [139.53767]\n",
      " [106.68896]\n",
      " [151.67944]\n",
      " [116.1338 ]\n",
      " [176.2008 ]\n",
      " [168.62495]\n",
      " [144.45981]\n",
      " [144.48346]\n",
      " [184.20947]\n",
      " [149.83134]\n",
      " [153.0279 ]\n",
      " [189.59079]\n",
      " [140.60085]\n",
      " [183.02943]\n",
      " [174.81757]\n",
      " [156.51384]\n",
      " [177.48946]\n",
      " [175.51476]\n",
      " [168.3035 ]\n",
      " [147.25967]\n",
      " [190.05975]] \n",
      "\n",
      "1200 Cost :  9.445965 \n",
      " Prediction: \n",
      "[[151.61092 ]\n",
      " [184.81999 ]\n",
      " [180.62398 ]\n",
      " [199.08238 ]\n",
      " [139.53336 ]\n",
      " [106.609146]\n",
      " [151.6435  ]\n",
      " [116.0619  ]\n",
      " [176.10893 ]\n",
      " [168.43794 ]\n",
      " [144.43852 ]\n",
      " [144.40218 ]\n",
      " [184.28883 ]\n",
      " [149.95073 ]\n",
      " [152.97218 ]\n",
      " [189.54123 ]\n",
      " [140.73454 ]\n",
      " [182.98297 ]\n",
      " [174.91533 ]\n",
      " [156.60448 ]\n",
      " [177.44794 ]\n",
      " [175.46277 ]\n",
      " [168.28438 ]\n",
      " [147.42464 ]\n",
      " [190.07616 ]] \n",
      "\n",
      "1300 Cost :  9.165836 \n",
      " Prediction: \n",
      "[[151.67566]\n",
      " [184.80263]\n",
      " [180.65923]\n",
      " [199.08049]\n",
      " [139.52965]\n",
      " [106.53304]\n",
      " [151.60883]\n",
      " [115.99252]\n",
      " [176.02135]\n",
      " [168.2592 ]\n",
      " [144.4181 ]\n",
      " [144.32457]\n",
      " [184.36488]\n",
      " [150.06514]\n",
      " [152.91867]\n",
      " [189.49397]\n",
      " [140.86288]\n",
      " [182.938  ]\n",
      " [175.00887]\n",
      " [156.69116]\n",
      " [177.40797]\n",
      " [175.4132 ]\n",
      " [168.26584]\n",
      " [147.58244]\n",
      " [190.0921 ]] \n",
      "\n",
      "1400 Cost :  8.908957 \n",
      " Prediction: \n",
      "[[151.73737]\n",
      " [184.78625]\n",
      " [180.69293]\n",
      " [199.07861]\n",
      " [139.52646]\n",
      " [106.46046]\n",
      " [151.57533]\n",
      " [115.92555]\n",
      " [175.93782]\n",
      " [168.08835]\n",
      " [144.3985 ]\n",
      " [144.25044]\n",
      " [184.43768]\n",
      " [150.1748 ]\n",
      " [152.86722]\n",
      " [189.44891]\n",
      " [140.98611]\n",
      " [182.89445]\n",
      " [175.09836]\n",
      " [156.77402]\n",
      " [177.36948]\n",
      " [175.3659 ]\n",
      " [168.24782]\n",
      " [147.73337]\n",
      " [190.10756]] \n",
      "\n",
      "1500 Cost :  8.673416 \n",
      " Prediction: \n",
      "[[151.79619]\n",
      " [184.77083]\n",
      " [180.72516]\n",
      " [199.07672]\n",
      " [139.52377]\n",
      " [106.39124]\n",
      " [151.543  ]\n",
      " [115.86091]\n",
      " [175.85818]\n",
      " [167.92503]\n",
      " [144.37965]\n",
      " [144.17966]\n",
      " [184.5074 ]\n",
      " [150.27988]\n",
      " [152.81776]\n",
      " [189.40594]\n",
      " [141.10439]\n",
      " [182.85226]\n",
      " [175.18398]\n",
      " [156.8532 ]\n",
      " [177.3324 ]\n",
      " [175.3208 ]\n",
      " [168.23032]\n",
      " [147.8777 ]\n",
      " [190.12256]] \n",
      "\n",
      "1600 Cost :  8.457407 \n",
      " Prediction: \n",
      "[[151.85225]\n",
      " [184.75632]\n",
      " [180.756  ]\n",
      " [199.07484]\n",
      " [139.52158]\n",
      " [106.32526]\n",
      " [151.51176]\n",
      " [115.79849]\n",
      " [175.78227]\n",
      " [167.76894]\n",
      " [144.36157]\n",
      " [144.11208]\n",
      " [184.57417]\n",
      " [150.3806 ]\n",
      " [152.77023]\n",
      " [189.36497]\n",
      " [141.21797]\n",
      " [182.8114 ]\n",
      " [175.26588]\n",
      " [156.92888]\n",
      " [177.29668]\n",
      " [175.27779]\n",
      " [168.2133 ]\n",
      " [148.01575]\n",
      " [190.13712]] \n",
      "\n",
      "1700 Cost :  8.259296 \n",
      " Prediction: \n",
      "[[151.90569]\n",
      " [184.74268]\n",
      " [180.78549]\n",
      " [199.07297]\n",
      " [139.51982]\n",
      " [106.26234]\n",
      " [151.4816 ]\n",
      " [115.73826]\n",
      " [175.70988]\n",
      " [167.61975]\n",
      " [144.34421]\n",
      " [144.04753]\n",
      " [184.63809]\n",
      " [150.47713]\n",
      " [152.72453]\n",
      " [189.32591]\n",
      " [141.32701]\n",
      " [182.77182]\n",
      " [175.34424]\n",
      " [157.00122]\n",
      " [177.26227]\n",
      " [175.23676]\n",
      " [168.19678]\n",
      " [148.14778]\n",
      " [190.15125]] \n",
      "\n",
      "1800 Cost :  8.077591 \n",
      " Prediction: \n",
      "[[151.95659]\n",
      " [184.72984]\n",
      " [180.81367]\n",
      " [199.0711 ]\n",
      " [139.51848]\n",
      " [106.20234]\n",
      " [151.45245]\n",
      " [115.68008]\n",
      " [175.64087]\n",
      " [167.47716]\n",
      " [144.32753]\n",
      " [143.98589]\n",
      " [184.69933]\n",
      " [150.56964]\n",
      " [152.68059]\n",
      " [189.28865]\n",
      " [141.4317 ]\n",
      " [182.73344]\n",
      " [175.41917]\n",
      " [157.07036]\n",
      " [177.22908]\n",
      " [175.19762]\n",
      " [168.18071]\n",
      " [148.27406]\n",
      " [190.16493]] \n",
      "\n",
      "1900 Cost :  7.910895 \n",
      " Prediction: \n",
      "[[152.00513 ]\n",
      " [184.7178  ]\n",
      " [180.84067 ]\n",
      " [199.06927 ]\n",
      " [139.51753 ]\n",
      " [106.145164]\n",
      " [151.42433 ]\n",
      " [115.62391 ]\n",
      " [175.5751  ]\n",
      " [167.34087 ]\n",
      " [144.31151 ]\n",
      " [143.92705 ]\n",
      " [184.75798 ]\n",
      " [150.65834 ]\n",
      " [152.63835 ]\n",
      " [189.25317 ]\n",
      " [141.53226 ]\n",
      " [182.6963  ]\n",
      " [175.4909  ]\n",
      " [157.13646 ]\n",
      " [177.19713 ]\n",
      " [175.16032 ]\n",
      " [168.16512 ]\n",
      " [148.39485 ]\n",
      " [190.17825 ]] \n",
      "\n",
      "2000 Cost :  7.757965 \n",
      " Prediction: \n",
      "[[152.05133]\n",
      " [184.70648]\n",
      " [180.86646]\n",
      " [199.06741]\n",
      " [139.51694]\n",
      " [106.09065]\n",
      " [151.39713]\n",
      " [115.56965]\n",
      " [175.5124 ]\n",
      " [167.21059]\n",
      " [144.29611]\n",
      " [143.87085]\n",
      " [184.81413]\n",
      " [150.74332]\n",
      " [152.59772]\n",
      " [189.21931]\n",
      " [141.62881]\n",
      " [182.66028]\n",
      " [175.55948]\n",
      " [157.19962]\n",
      " [177.16632]\n",
      " [175.12473]\n",
      " [168.14995]\n",
      " [148.51035]\n",
      " [190.19115]] \n",
      "\n",
      "Your score will be  [[190.0065]]\n",
      "Other score will be  [[196.85834]\n",
      " [171.07678]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# loadtxt 이용하기 단점은 전체의 data가 같은 data type이어야한다.\n",
    "xy = np.loadtxt('data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "# x와 y를 나누어야하는데 이 때 python의 기능 중 slice가 필요하다.\n",
    "# python Slicing 공부하기\n",
    "# numpy에서는 더욱 막강한 slicing이 가능해진다! 찾아서 공부하기\n",
    "\n",
    "#: 전체 행을 다 가져온다, 0:-1 마지막꺼 전까지 가져오겠다.\n",
    "x_data = xy[:, 0:-1]\n",
    "#: 전체 행을 가져오는데 마지막 행만 가져오겠다.\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# cost/Los function\n",
    "# 이 전에 쓰던 것과 동일함\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "#Minimize learning_rate를 낮게 줌\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    #train을 돌리고 그 때, cost와 hypothes를 같이 기록을 해서 출력을 하는 모델\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                                  feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 100 == 0:\n",
    "        print step ,'Cost : ', cost_val,\"\\n Prediction: \\n\", hy_val,\"\\n\"\n",
    "              \n",
    "\n",
    "# Ask my score\n",
    "print \"Your score will be \", sess.run(hypothesis,\n",
    "                                    feed_dict={X: [[100, 70, 101]]})\n",
    "print \"Other score will be \", sess.run(hypothesis,\n",
    "                                      feed_dict={X: [[60, 80, 110], [90, 100, 80]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queue Runners\n",
    "\n",
    "File이 굉장히 커서 memory에 한 번에 올리기도 힘이든 경우에\n",
    "numpy를 이용하면 memory가 부족하다고 나온다.  \n",
    "<br>\n",
    "이런 경우를 대비해서 TensorFlow에서는 **Queue Runner**라는 시스템을 제공한다.  \n",
    "기본적으로 여러개의 파일을 읽을 수 있다.\n",
    "<br>\n",
    "가장 간단하게 load해오는 방법은 numpy에 loadtxt를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-7160b8eb033f>:3: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:197: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-3-7160b8eb033f>:5: __init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TextLineDataset`.\n",
      "WARNING:tensorflow:From <ipython-input-3-7160b8eb033f>:13: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From <ipython-input-3-7160b8eb033f>:38: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "0 Cost:  1600.8619 \n",
      "Prediction:\n",
      "[[111.37641 ]\n",
      " [142.95267 ]\n",
      " [136.20432 ]\n",
      " [147.05948 ]\n",
      " [112.49426 ]\n",
      " [ 82.335976]\n",
      " [105.86509 ]\n",
      " [ 72.24987 ]\n",
      " [134.30702 ]\n",
      " [122.59853 ]]\n",
      "200 Cost:  36.08012 \n",
      "Prediction:\n",
      "[[150.61201 ]\n",
      " [189.79816 ]\n",
      " [182.51024 ]\n",
      " [197.64276 ]\n",
      " [148.01808 ]\n",
      " [108.725395]\n",
      " [144.04317 ]\n",
      " [101.00201 ]\n",
      " [178.20161 ]\n",
      " [163.5652  ]]\n",
      "400 Cost:  33.026115 \n",
      "Prediction:\n",
      "[[150.69992]\n",
      " [189.59085]\n",
      " [182.45566]\n",
      " [197.7244 ]\n",
      " [147.6658 ]\n",
      " [108.64688]\n",
      " [144.40207]\n",
      " [101.69363]\n",
      " [178.10774]\n",
      " [163.76222]]\n",
      "600 Cost:  30.268167 \n",
      "Prediction:\n",
      "[[150.78816 ]\n",
      " [189.39114 ]\n",
      " [182.40562 ]\n",
      " [197.8017  ]\n",
      " [147.32874 ]\n",
      " [108.56612 ]\n",
      " [144.74191 ]\n",
      " [102.348785]\n",
      " [178.01128 ]\n",
      " [163.937   ]]\n",
      "800 Cost:  27.776653 \n",
      "Prediction:\n",
      "[[150.87633 ]\n",
      " [189.19887 ]\n",
      " [182.35977 ]\n",
      " [197.87492 ]\n",
      " [147.00632 ]\n",
      " [108.483734]\n",
      " [145.06377 ]\n",
      " [102.96953 ]\n",
      " [177.91298 ]\n",
      " [164.09169 ]]\n",
      "1000 Cost:  25.525288 \n",
      "Prediction:\n",
      "[[150.96407]\n",
      " [189.0139 ]\n",
      " [182.31772]\n",
      " [197.94432]\n",
      " [146.698  ]\n",
      " [108.4003 ]\n",
      " [145.36864]\n",
      " [103.55773]\n",
      " [177.81355]\n",
      " [164.22833]]\n",
      "1200 Cost:  23.490437 \n",
      "Prediction:\n",
      "[[151.05104 ]\n",
      " [188.83604 ]\n",
      " [182.27916 ]\n",
      " [198.01006 ]\n",
      " [146.40324 ]\n",
      " [108.316315]\n",
      " [145.65747 ]\n",
      " [104.11517 ]\n",
      " [177.71352 ]\n",
      " [164.3487  ]]\n",
      "1400 Cost:  21.650969 \n",
      "Prediction:\n",
      "[[151.13701]\n",
      " [188.66513]\n",
      " [182.24382]\n",
      " [198.07239]\n",
      " [146.1215 ]\n",
      " [108.23219]\n",
      " [145.93114]\n",
      " [104.64353]\n",
      " [177.61342]\n",
      " [164.45442]]\n",
      "1600 Cost:  19.98779 \n",
      "Prediction:\n",
      "[[151.22173 ]\n",
      " [188.50098 ]\n",
      " [182.21138 ]\n",
      " [198.13145 ]\n",
      " [145.85226 ]\n",
      " [108.14833 ]\n",
      " [146.19049 ]\n",
      " [105.144424]\n",
      " [177.51367 ]\n",
      " [164.547   ]]\n",
      "1800 Cost:  18.483894 \n",
      "Prediction:\n",
      "[[151.30496 ]\n",
      " [188.34335 ]\n",
      " [182.18163 ]\n",
      " [198.18747 ]\n",
      " [145.59502 ]\n",
      " [108.065025]\n",
      " [146.4363  ]\n",
      " [105.61931 ]\n",
      " [177.41466 ]\n",
      " [164.62776 ]]\n",
      "2000 Cost:  17.123972 \n",
      "Prediction:\n",
      "[[151.3866 ]\n",
      " [188.19211]\n",
      " [182.15431]\n",
      " [198.24057]\n",
      " [145.3493 ]\n",
      " [107.98257]\n",
      " [146.66931]\n",
      " [106.06959]\n",
      " [177.31671]\n",
      " [164.6979 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "['data-01-test-score.csv', 'data-02-test-score.csv'], shuffle=False, name='filename_queue')\n",
    "\n",
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "# decoded result\n",
    "record_defaults = [[0.], [0.], [0.], [0.]]\n",
    "xy = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "\n",
    "# collect batches of csv in\n",
    "train_x_batch, train_y_batch = \\\n",
    "tf.train.batch([xy[0:-1], xy[-1:]], batch_size=10)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# cost/Los function\n",
    "# 이 전에 쓰던 것과 동일함\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "#Minimize learning_rate를 낮게 줌\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Start populating the filename queue\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "for step in range(2001):\n",
    "    x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "    [cost, hypothesis, train],\n",
    "    feed_dict={X: x_batch, Y: y_batch})\n",
    "    \n",
    "    if step % 200 == 0:\n",
    "        print step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val\n",
    "        \n",
    "\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
